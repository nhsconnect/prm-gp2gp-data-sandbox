{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "400d815e",
   "metadata": {},
   "source": [
    "### PRMT-2039 [\"HYPOTHESIS\"] Table showing distribution number of messages per pathway and status\n",
    "\n",
    "We have so far assumed that a particular transfer status is associated with a particular number of messages:\n",
    "Eg Pending would typically be expected to have 3, Integrated would usually expect to see 4.\n",
    "\n",
    "However, we have seen fewer that 3 for pending in inspection of some Vision to Vision transfers.\n",
    "\n",
    "By generating a full table:\n",
    "- for 6 months of transfers, \n",
    "- broken down by status and supplier pathway, \n",
    "- showing what % of transfers had what number of messages, \n",
    "\n",
    "we can:\n",
    "- check the degree to which this assumption holds. \n",
    "- Identify areas (eg by pathway and/or status) for further investigation\n",
    "- This may allow us to redefine our statuses from their current 4\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59ab7a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Using data generated from branch PRMT-1742-duplicates-analysis.\n",
    "# This is needed to correctly handle duplicates.\n",
    "# Once the upstream pipeline has a fix for duplicate EHRs, then we can go back to using the main output.\n",
    "transfer_file_location = \"s3://prm-gp2gp-data-sandbox-dev/transfers-duplicates-hypothesis/\"\n",
    "transfer_files = [\n",
    "    \"9-2020-transfers.parquet\",\n",
    "    \"10-2020-transfers.parquet\",\n",
    "    \"11-2020-transfers.parquet\",\n",
    "    \"12-2020-transfers.parquet\",\n",
    "    \"1-2021-transfers.parquet\",\n",
    "    \"2-2021-transfers.parquet\"\n",
    "]\n",
    "\n",
    "transfer_input_files = [transfer_file_location + f for f in transfer_files]\n",
    "transfers_raw = pd.concat((\n",
    "    pd.read_parquet(f)\n",
    "    for f in transfer_input_files\n",
    "))\n",
    "\n",
    "# In the data from the PRMT-1742-duplicates-analysis branch, these columns have been added , but contain only empty values.\n",
    "transfers_raw = transfers_raw.drop([\"sending_supplier\", \"requesting_supplier\"], axis=1)\n",
    "\n",
    "\n",
    "# Given the findings in PRMT-1742 - many duplicate EHR errors are misclassified, the below reclassifies the relevant data\n",
    "\n",
    "has_at_least_one_successful_integration_code = lambda errors: any((np.isnan(e) or e==15 for e in errors))\n",
    "successful_transfers_bool = transfers_raw['request_completed_ack_codes'].apply(has_at_least_one_successful_integration_code)\n",
    "transfers = transfers_raw.copy()\n",
    "transfers.loc[successful_transfers_bool, \"status\"] = \"INTEGRATED\"\n",
    "\n",
    "# Correctly interpret certail sender errors as failed.\n",
    "# This is explained in PRMT-1974. Eventaully this will be fixed upstream in the pipeline. \n",
    "pending_sender_error_codes=[6,7,10,24,30,23,14,99]\n",
    "transfers_with_pending_sender_code_bool=transfers['sender_error_code'].isin(pending_sender_error_codes)\n",
    "transfers_with_pending_with_error_bool=transfers['status']=='PENDING_WITH_ERROR'\n",
    "transfers_which_need_pending_to_failure_change_bool=transfers_with_pending_sender_code_bool & transfers_with_pending_with_error_bool\n",
    "transfers.loc[transfers_which_need_pending_to_failure_change_bool,'status']='FAILED'\n",
    "\n",
    "# Add integrated Late status\n",
    "eight_days_in_seconds=8*24*60*60\n",
    "transfers_after_sla_bool=transfers['sla_duration']>eight_days_in_seconds\n",
    "transfers_with_integrated_bool=transfers['status']=='INTEGRATED'\n",
    "transfers_integrated_late_bool=transfers_after_sla_bool & transfers_with_integrated_bool\n",
    "transfers.loc[transfers_integrated_late_bool,'status']='INTEGRATED LATE'\n",
    "\n",
    "# If the record integrated after 28 days, change the status back to pending.\n",
    "# This is to handle each month consistentently and to always reflect a transfers status 28 days after it was made.\n",
    "# TBD how this is handled upstream in the pipeline\n",
    "twenty_eight_days_in_seconds=28*24*60*60\n",
    "transfers_after_month_bool=transfers['sla_duration']>twenty_eight_days_in_seconds\n",
    "transfers_pending_at_month_bool=transfers_after_month_bool & transfers_integrated_late_bool\n",
    "transfers.loc[transfers_pending_at_month_bool,'status']='PENDING'\n",
    "transfers_with_early_error_bool=(~transfers.loc[:,'sender_error_code'].isna()) |(~transfers.loc[:,'intermediate_error_codes'].apply(len)>0)\n",
    "transfers.loc[transfers_with_early_error_bool & transfers_pending_at_month_bool,'status']='PENDING_WITH_ERROR'\n",
    "\n",
    "# Supplier name mapping\n",
    "supplier_renaming = {\n",
    "    \"EGTON MEDICAL INFORMATION SYSTEMS LTD (EMIS)\":\"EMIS\",\n",
    "    \"IN PRACTICE SYSTEMS LTD\":\"Vision\",\n",
    "    \"MICROTEST LTD\":\"Microtest\",\n",
    "    \"THE PHOENIX PARTNERSHIP\":\"TPP\",\n",
    "    None: \"Unknown\"\n",
    "}\n",
    "\n",
    "asid_lookup_file = \"s3://prm-gp2gp-data-sandbox-dev/asid-lookup/asidLookup-Mar-2021.csv.gz\"\n",
    "asid_lookup = pd.read_csv(asid_lookup_file)\n",
    "lookup = asid_lookup[[\"ASID\", \"MName\", \"NACS\",\"OrgName\"]]\n",
    "\n",
    "transfers = transfers.merge(lookup, left_on='requesting_practice_asid',right_on='ASID',how='left')\n",
    "transfers = transfers.rename({'MName': 'requesting_supplier', 'ASID': 'requesting_supplier_asid', 'NACS': 'requesting_ods_code','OrgName':'requesting_practice_name'}, axis=1)\n",
    "transfers = transfers.merge(lookup, left_on='sending_practice_asid',right_on='ASID',how='left')\n",
    "transfers = transfers.rename({'MName': 'sending_supplier', 'ASID': 'sending_supplier_asid', 'NACS': 'sending_ods_code','OrgName':'sending_practice_name'}, axis=1)\n",
    "\n",
    "transfers[\"sending_supplier\"] = transfers[\"sending_supplier\"].replace(supplier_renaming.keys(), supplier_renaming.values())\n",
    "transfers[\"requesting_supplier\"] = transfers[\"requesting_supplier\"].replace(supplier_renaming.keys(), supplier_renaming.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a03a0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the message data\n",
    "interaction_file_name='s3://prm-gp2gp-data-sandbox-dev/extra-fields-data-from-splunk/Sept_20_Feb_21_conversations_interaction_messages.parquet'\n",
    "message_lists=pd.read_parquet(interaction_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bed8ebed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the datasets\n",
    "combined_transfers=transfers.merge(message_lists,left_on='conversation_id',right_index=True ,how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11100efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in the number of messages as the main field\n",
    "combined_transfers['interaction length']=combined_transfers['interaction name'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5d7f68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a table to count the transfers of each message length broken down by pathway/status\n",
    "message_count_table=combined_transfers.pivot_table(index=['requesting_supplier','sending_supplier','status'],columns='interaction length',values='conversation_id',aggfunc='count').fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d2db7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each pathway/status, what is the percentage in each message length?\n",
    "message_count_table_percentages=message_count_table.div(message_count_table.sum(axis=1),axis=0).multiply(100).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "808d0d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_count_table_percentages.reset_index().to_csv('s3://prm-gp2gp-data-sandbox-dev/notebook-outputs/34--PRMT-2039-message-length-counts-by-pathway-and-status.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
