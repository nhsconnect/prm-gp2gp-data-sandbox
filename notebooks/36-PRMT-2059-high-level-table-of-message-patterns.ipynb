{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f90117e",
   "metadata": {},
   "source": [
    "# PRMT-2059 Generate high level table of message patterns prevalance\n",
    "We believe that being able to break down transfers by the set of messages that occur between the sending and receiving supplier will give us a better understanding of what the actual status of the transfer is. \n",
    "\n",
    "In particular, Pending transfers may have a technical issue or may be awaiting practice integration - we may be able to distinguish between these. \n",
    "For 6 months of transfers (September 2020 to Feb 2021), we wish to be able to see the list of messages in the form:\n",
    "- The message creator (sending or requesting practice)\n",
    "- The message type (interaction name)\n",
    "- Any associate code (jdi event)\n",
    "\n",
    "We then wish to break down all transfers by:\n",
    "- The supplier pathway\n",
    "- The Status\n",
    "- The message chain\n",
    "And order these in a table according to how common they are\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88da2aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c67d8e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite_files = True\n",
    "\n",
    "# Turn on the option to only include the first pair of messages\n",
    "# where multiple COPC messages with successful acknowledgements were sent in a single transfer\n",
    "reduce_COPC_messages = True\n",
    "\n",
    "COPC_tag = \"-reduced-COPCs\" if reduce_COPC_messages else \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a73e49",
   "metadata": {},
   "source": [
    "## Importing transfer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2255623b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import transfer files to extract whether message creator is sender or requester\n",
    "# Using data generated from branch PRMT-1742-duplicates-analysis.\n",
    "# This is needed to correctly handle duplicates.\n",
    "# Once the upstream pipeline has a fix for duplicate EHRs, then we can go back to using the main output.\n",
    "transfer_file_location = \"s3://prm-gp2gp-data-sandbox-dev/transfers-duplicates-hypothesis/\"\n",
    "transfer_files = [\n",
    "    \"9-2020-transfers.parquet\",\n",
    "    \"10-2020-transfers.parquet\",\n",
    "    \"11-2020-transfers.parquet\",\n",
    "    \"12-2020-transfers.parquet\",\n",
    "    \"1-2021-transfers.parquet\",\n",
    "    \"2-2021-transfers.parquet\"\n",
    "]\n",
    "\n",
    "transfer_input_files = [transfer_file_location + f for f in transfer_files]\n",
    "transfers_raw = pd.concat((\n",
    "    pd.read_parquet(f)\n",
    "    for f in transfer_input_files\n",
    "))\n",
    "\n",
    "# In the data from the PRMT-1742-duplicates-analysis branch, these columns have been added , but contain only empty values.\n",
    "transfers_raw = transfers_raw.drop([\"sending_supplier\", \"requesting_supplier\"], axis=1)\n",
    "\n",
    "# Given the findings in PRMT-1742 - many duplicate EHR errors are misclassified, the below reclassifies the relevant data\n",
    "has_at_least_one_successful_integration_code = lambda errors: any((np.isnan(e) or e==15 for e in errors))\n",
    "successful_transfers_bool = transfers_raw['request_completed_ack_codes'].apply(has_at_least_one_successful_integration_code)\n",
    "transfers = transfers_raw.copy()\n",
    "transfers.loc[successful_transfers_bool, \"status\"] = \"INTEGRATED\"\n",
    "\n",
    "# Correctly interpret certain sender errors as failed.\n",
    "# This is explained in PRMT-1974. Eventually this will be fixed upstream in the pipeline.\n",
    "pending_sender_error_codes=[6,7,10,24,30,23,14,99]\n",
    "transfers_with_pending_sender_code_bool=transfers['sender_error_code'].isin(pending_sender_error_codes)\n",
    "transfers_with_pending_with_error_bool=transfers['status']=='PENDING_WITH_ERROR'\n",
    "transfers_which_need_pending_to_failure_change_bool=transfers_with_pending_sender_code_bool & transfers_with_pending_with_error_bool\n",
    "transfers.loc[transfers_which_need_pending_to_failure_change_bool,'status']='FAILED'\n",
    "\n",
    "# Add integrated Late status\n",
    "eight_days_in_seconds=8*24*60*60\n",
    "transfers_after_sla_bool=transfers['sla_duration']>eight_days_in_seconds\n",
    "transfers_with_integrated_bool=transfers['status']=='INTEGRATED'\n",
    "transfers_integrated_late_bool=transfers_after_sla_bool & transfers_with_integrated_bool\n",
    "transfers.loc[transfers_integrated_late_bool,'status']='INTEGRATED LATE'\n",
    "\n",
    "# If the record integrated after 28 days, change the status back to pending.\n",
    "# This is to handle each month consistently and to always reflect a transfers status 28 days after it was made.\n",
    "# TBD how this is handled upstream in the pipeline\n",
    "twenty_eight_days_in_seconds=28*24*60*60\n",
    "transfers_after_month_bool=transfers['sla_duration']>twenty_eight_days_in_seconds\n",
    "transfers_pending_at_month_bool=transfers_after_month_bool & transfers_integrated_late_bool\n",
    "transfers.loc[transfers_pending_at_month_bool,'status']='PENDING'\n",
    "transfers_with_early_error_bool=(~transfers.loc[:,'sender_error_code'].isna()) |(~transfers.loc[:,'intermediate_error_codes'].apply(len)>0)\n",
    "transfers.loc[transfers_with_early_error_bool & transfers_pending_at_month_bool,'status']='PENDING_WITH_ERROR'\n",
    "\n",
    "# Supplier name mapping\n",
    "supplier_renaming = {\n",
    "    \"EGTON MEDICAL INFORMATION SYSTEMS LTD (EMIS)\":\"EMIS\",\n",
    "    \"IN PRACTICE SYSTEMS LTD\":\"Vision\",\n",
    "    \"MICROTEST LTD\":\"Microtest\",\n",
    "    \"THE PHOENIX PARTNERSHIP\":\"TPP\",\n",
    "    None: \"Unknown\"\n",
    "}\n",
    "\n",
    "asid_lookup_file = \"s3://prm-gp2gp-data-sandbox-dev/asid-lookup/asidLookup-Mar-2021.csv.gz\"\n",
    "asid_lookup = pd.read_csv(asid_lookup_file)\n",
    "lookup = asid_lookup[[\"ASID\", \"MName\", \"NACS\",\"OrgName\"]]\n",
    "\n",
    "transfers = transfers.merge(lookup, left_on='requesting_practice_asid',right_on='ASID',how='left')\n",
    "transfers = transfers.rename({'MName': 'requesting_supplier', 'ASID': 'requesting_supplier_asid', 'NACS': 'requesting_ods_code','OrgName':'requesting_practice_name'}, axis=1)\n",
    "transfers = transfers.merge(lookup, left_on='sending_practice_asid',right_on='ASID',how='left')\n",
    "transfers = transfers.rename({'MName': 'sending_supplier', 'ASID': 'sending_supplier_asid', 'NACS': 'sending_ods_code','OrgName':'sending_practice_name'}, axis=1)\n",
    "\n",
    "transfers[\"sending_supplier\"] = transfers[\"sending_supplier\"].replace(supplier_renaming.keys(), supplier_renaming.values())\n",
    "transfers[\"requesting_supplier\"] = transfers[\"requesting_supplier\"].replace(supplier_renaming.keys(), supplier_renaming.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40910c7",
   "metadata": {},
   "source": [
    "## Stage 1\n",
    " \n",
    "Using the raw Spine data for the transfers above, we want to generate the list of messages in the form:\n",
    "- The message creator (sending or requesting practice)\n",
    "- The message type (interaction name)\n",
    "    - in the case of application acknowledgement message, we wish to see what type of message it is acknowledging\n",
    "- Any associate code (JDI event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "296f360c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a mapping of practice asid, and whether they were the sender or requestor in that conversation\n",
    "requesting_supplier_type_map = transfers[[\"conversation_id\", \"requesting_practice_asid\", \"date_requested\"]].drop_duplicates()\n",
    "sending_supplier_type_map = transfers[[\"conversation_id\", \"sending_practice_asid\", \"date_requested\"]].drop_duplicates()\n",
    "\n",
    "requesting_supplier_type_map[\"supplier_type\"] = \"requestor\"\n",
    "sending_supplier_type_map[\"supplier_type\"] = \"sender\"\n",
    "\n",
    "requesting_supplier_type_map = requesting_supplier_type_map.rename({\"requesting_practice_asid\": \"practice_asid\"}, axis=1)\n",
    "sending_supplier_type_map = sending_supplier_type_map.rename({\"sending_practice_asid\": \"practice_asid\"}, axis=1)\n",
    "\n",
    "supplier_type_mapping = pd.concat([requesting_supplier_type_map, sending_supplier_type_map])\n",
    "supplier_type_mapping[\"practice_asid\"] = supplier_type_mapping[\"practice_asid\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebd9b88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_ids_of_interest=transfers['conversation_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d96ccea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of files to be loaded in\n",
    "#folder=\"s3://prm-gp2gp-data-sandbox-dev/spine-gp2gp-data-with-ack-codes-prmt-2059/\"\n",
    "folder=\"s3://prm-gp2gp-data-sandbox-dev/spine-gp2gp-data/\"\n",
    "files=[\"Sept-2020\",\"Oct-2020\",\"Nov-2020\",\"Dec-2020\",\"Jan-2021\",\"Feb-2021\",\"Mar-2021\"]\n",
    "full_filenames=[folder + file + \".csv.gz\" for file in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7799c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename message types to be human readable\n",
    "interaction_name_mapping={\"urn:nhs:names:services:gp2gp/RCMR_IN010000UK05\":\"req start\",\n",
    "\"urn:nhs:names:services:gp2gp/RCMR_IN030000UK06\":\"req complete\",\n",
    "\"urn:nhs:names:services:gp2gp/COPC_IN000001UK01\":\"COPC\",\n",
    "\"urn:nhs:names:services:gp2gp/MCCI_IN010000UK13\":\" ack\"}\n",
    "\n",
    "#ackTypeCode_mapping={'AE':\"Neg\",\"AR\":\"Neg\",\"ER\":\"Neg\",\"IF\":\"Pos\",\"NONE\":\"Pos\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5856a135",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc2ca773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will take a set of Spine data and for each message in the conversation we're interested in, it will output a dataframe\n",
    "# with \"conversation_id\", \"supplier_type\", \"interaction_name\", \"jdiEvent\", \"GUID\", \"messageRef\" for each given message in the order\n",
    "# they occur\n",
    "def generate_single_frame(file):\n",
    "    a=time.perf_counter()\n",
    "    print(\"Now Processing \" + file)\n",
    "    df=pd.read_csv(file, compression='gzip',error_bad_lines=False)\n",
    "\n",
    "    # Only keep conversations from the conversations that we actually want to use\n",
    "    df=df.loc[df['conversationID'].isin(conversation_ids_of_interest)]\n",
    "    df=df.sort_values(by='_time')\n",
    "    df = df.merge(supplier_type_mapping, left_on=[\"conversationID\", \"messageSender\"], right_on=[\"conversation_id\", \"practice_asid\"], how=\"left\")\n",
    "    \n",
    "    # filter out messages that took place more than 28 days after the date requested\n",
    "    in_time_message_bool = (pd.to_datetime(df[\"_time\"]).dt.tz_localize(None) - df[\"date_requested\"]).dt.total_seconds() <= twenty_eight_days_in_seconds\n",
    "    df = df.loc[in_time_message_bool]\n",
    "    \n",
    "    # map the message name to human readable form using supplier mapping\n",
    "    df['interaction_name']=df['interactionID'].replace(interaction_name_mapping)\n",
    "    df[\"jdiEvent\"] = df[\"jdiEvent\"].replace(\"NONE\", \"\")\n",
    "    \n",
    "    df=df[[\"conversation_id\",\"supplier_type\",\"interaction_name\",\"jdiEvent\",\"GUID\",\"messageRef\"]]\n",
    "    print(time.perf_counter()-a)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "baa8a5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now Processing s3://prm-gp2gp-data-sandbox-dev/spine-gp2gp-data/Sept-2020.csv.gz\n",
      "62.93144888099778\n",
      "Now Processing s3://prm-gp2gp-data-sandbox-dev/spine-gp2gp-data/Oct-2020.csv.gz\n",
      "54.177169143000356\n",
      "Now Processing s3://prm-gp2gp-data-sandbox-dev/spine-gp2gp-data/Nov-2020.csv.gz\n",
      "49.25512907700249\n",
      "Now Processing s3://prm-gp2gp-data-sandbox-dev/spine-gp2gp-data/Dec-2020.csv.gz\n",
      "46.31936260099974\n",
      "Now Processing s3://prm-gp2gp-data-sandbox-dev/spine-gp2gp-data/Jan-2021.csv.gz\n",
      "51.915243125000416\n",
      "Now Processing s3://prm-gp2gp-data-sandbox-dev/spine-gp2gp-data/Feb-2021.csv.gz\n",
      "54.136961001997406\n",
      "Now Processing s3://prm-gp2gp-data-sandbox-dev/spine-gp2gp-data/Mar-2021.csv.gz\n",
      "45.43095088300106\n",
      "Now Concatenating all months of data\n",
      "3.5775846779979474\n"
     ]
    }
   ],
   "source": [
    "df=[generate_single_frame(file) for file in full_filenames]\n",
    "\n",
    "print('Now Concatenating all months of data')\n",
    "a=time.perf_counter()\n",
    "df=pd.concat(df,axis=0)\n",
    "print(time.perf_counter()-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55d17fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only include the first pair of messages where multiple COPC messages with successful acknowledgements were sent in a single transfer\n",
    "# The following code identifies indexes of messages we want to remove in df, and then removes them\n",
    "if reduce_COPC_messages:\n",
    "    df=df.reset_index(drop=True)\n",
    "    COPC_data=df.copy()\n",
    "\n",
    "    # Sender COPCs\n",
    "    COPCs_bool=(COPC_data['supplier_type']=='sender') & (COPC_data['interaction_name']=='COPC')\n",
    "    COPC_data=COPC_data.loc[COPCs_bool].reset_index().rename({'index':'Sender COPC index'},axis=1)\n",
    "\n",
    "    # Requestor COPC ack\n",
    "    COPC_data=COPC_data.merge(df[['messageRef','interaction_name', 'jdiEvent']].reset_index().rename({'index':'Requestor COPC ack index'},axis=1),left_on='GUID',right_on='messageRef',how='inner')\n",
    "\n",
    "    # Filter out anything with a negative acknowledgement (ie a JDI event)\n",
    "    successful_responses_bool=COPC_data['jdiEvent_y']==\"\"\n",
    "    COPC_data=COPC_data.loc[successful_responses_bool]\n",
    "\n",
    "    COPC_data=COPC_data[['conversation_id','Sender COPC index','Requestor COPC ack index']].groupby('conversation_id').agg(list)\n",
    "\n",
    "    multiple_COPC_message_conversations_bool=COPC_data['Sender COPC index'].apply(len)>1\n",
    "    COPC_data=COPC_data.loc[multiple_COPC_message_conversations_bool]\n",
    "    messages_to_remove=COPC_data.apply(lambda row: row['Sender COPC index'][1:]+row['Requestor COPC ack index'][1:],axis=1).explode().values\n",
    "    df=df.drop(messages_to_remove,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d58adb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now constructing full interactions\n",
      "31.483020542000304\n",
      "Now Grouping by conversation\n",
      "42.34794190899993\n",
      "Now Saving Data\n"
     ]
    }
   ],
   "source": [
    "print('Now constructing full interactions')\n",
    "a=time.perf_counter()\n",
    "df2=df.merge(df[['GUID','interaction_name']].rename({'interaction_name':'interaction_response'},axis=1),left_on='messageRef',right_on='GUID',how='left')\n",
    "df2['interaction_response']=df2['interaction_response'].fillna(\"\")\n",
    "df2['interaction']=df2['interaction_response']+df2['interaction_name']\n",
    "df2[\"messages\"] = list(zip(df2[\"supplier_type\"], df2[\"interaction\"], df2[\"jdiEvent\"]))\n",
    "df2[\"messages\"] = df2[\"messages\"].apply(list)\n",
    "df2=df2[[\"conversation_id\", \"messages\"]]\n",
    "df2\n",
    "print(time.perf_counter()-a)\n",
    "\n",
    "print('Now Grouping by conversation')\n",
    "a=time.perf_counter()\n",
    "full_field_data=df2.groupby('conversation_id')['messages'].apply(list)\n",
    "print(time.perf_counter()-a)\n",
    "\n",
    "if overwrite_files:\n",
    "    print('Now Saving Data')\n",
    "    pd.DataFrame(full_field_data).to_parquet(f's3://prm-gp2gp-data-sandbox-dev/extra-fields-data-from-splunk/Sept_20_Feb_21_conversations_extended_interaction_messages{COPC_tag}.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b5cc7a",
   "metadata": {},
   "source": [
    "## Stage 2\n",
    "\n",
    "Create csv files, which show the most common supplier pathway, status and message list combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2df522a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations_extended_interaction_messages=pd.read_parquet(f's3://prm-gp2gp-data-sandbox-dev/extra-fields-data-from-splunk/Sept_20_Feb_21_conversations_extended_interaction_messages{COPC_tag}.parquet')\n",
    "# turning messages from list of list to tuple of tuples (since they are hasable)\n",
    "conversations_extended_interaction_messages[\"messages\"]=conversations_extended_interaction_messages[\"messages\"].apply(lambda message_list: tuple([tuple(message) for message in message_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7c05375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach this message list to the transfers dataframe\n",
    "transfers_with_message_list = transfers.merge(conversations_extended_interaction_messages, left_on=\"conversation_id\", right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85fcac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of transfers per status and supplier pathway and message pattern combination\n",
    "message_list_prevelance_table = transfers_with_message_list.groupby([\"sending_supplier\", \"requesting_supplier\", \"status\", \"messages\"]).agg({\"conversation_id\": \"count\"})\n",
    "message_list_prevelance_table = message_list_prevelance_table.rename({\"conversation_id\": \"Total Number of transfers\"}, axis=1).sort_values(by=\"Total Number of transfers\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d015f80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_transfer_count = message_list_prevelance_table[\"Total Number of transfers\"].sum()\n",
    "message_list_prevelance_table[\"% Transfers\"] = (message_list_prevelance_table[\"Total Number of transfers\"] / total_transfer_count).multiply(100).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcf832d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's add a column with the percentage of transfers for that combination of Supplier Pathway and status\n",
    "\n",
    "# The columns we are aggregating on (ie supplier pathway and status)\n",
    "column_interested_in = [\"sending_supplier\", \"requesting_supplier\", \"status\"]\n",
    "\n",
    "# Create a table of the count of transfers for each of these supplier pathways\n",
    "pathway_and_status_counts = transfers_with_message_list.groupby(column_interested_in).agg({\"conversation_id\": \"count\"})\n",
    "pathway_and_status_counts = pathway_and_status_counts.rename({\"conversation_id\": \"Pathway and status totals\"}, axis=1)\n",
    "\n",
    "# Take the relevant indexes from our original table and use this to get a full list of the number of transfers for each row's pathway and status\n",
    "order_of_indexes_needed=message_list_prevelance_table.reset_index().set_index(column_interested_in).index\n",
    "ordered_totals=pathway_and_status_counts.loc[order_of_indexes_needed]\n",
    "\n",
    "# Divide the transfers by these values to get the percentage\n",
    "message_list_prevelance_table[\"% Pathway and status transfers\"] =(message_list_prevelance_table['Total Number of transfers'].values/ordered_totals['Pathway and status totals'].values)\n",
    "message_list_prevelance_table[\"% Pathway and status transfers\"] =message_list_prevelance_table[\"% Pathway and status transfers\"].multiply(100).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c2012fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3944, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_list_prevelance_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2149d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1635, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering out the combinations that only have one transfer associated\n",
    "reduced_message_list_prevelance_table_bool = message_list_prevelance_table[\"Total Number of transfers\"] > 1\n",
    "reduced_message_list_prevelance_table = message_list_prevelance_table[reduced_message_list_prevelance_table_bool]\n",
    "reduced_message_list_prevelance_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39931ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: there are 2309 combinations of supplier pathway, status and message pattern with only 1 transfer associated, which we have filtered out\n"
     ]
    }
   ],
   "source": [
    "print(f\"Note: there are {message_list_prevelance_table.shape[0] - reduced_message_list_prevelance_table.shape[0]} combinations of supplier pathway, status and message pattern with only 1 transfer associated, which we have filtered out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cdbbb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if overwrite_files:\n",
    "    pd.DataFrame(reduced_message_list_prevelance_table).to_csv(f's3://prm-gp2gp-data-sandbox-dev/notebook-outputs/36--PRMT-2059-high-level-table-of-message-patterns-reduced{COPC_tag}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
