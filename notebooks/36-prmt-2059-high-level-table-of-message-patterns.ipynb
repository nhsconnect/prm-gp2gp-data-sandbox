{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75e2520c",
   "metadata": {},
   "source": [
    "# PRMT-2059 Generate high level table of message patterns prevalance\n",
    "We believe that being able to break down transfers by the set of messages that occur between the sending and receiving supplier will give us a better understanding of what the actual status of the transfer is. \n",
    "\n",
    "In particular, Pending transfers may have a technical issue or may be awaiting practice integration - we may be able to distinguish between these. \n",
    "For 6 months of transfers (September 2020 to Feb 2021), we wish to be able to see the list of messages in the form:\n",
    "- The message creator (sending or requesting practice)\n",
    "- The message type (interaction name)\n",
    "- Any associate code (jdi event)\n",
    "\n",
    "We then wish to break down all transfers by:\n",
    "- The supplier pathway\n",
    "- The Status\n",
    "- The message chain\n",
    "And order these in a table according to how common they are\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db669b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a2a70e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite_files = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986d49c1",
   "metadata": {},
   "source": [
    "## Importing transfer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be517775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import transfer files to extract whether message creator is sender or requester\n",
    "# Using data generated from branch PRMT-1742-duplicates-analysis.\n",
    "# This is needed to correctly handle duplicates.\n",
    "# Once the upstream pipeline has a fix for duplicate EHRs, then we can go back to using the main output.\n",
    "transfer_file_location = \"s3://prm-gp2gp-data-sandbox-dev/transfers-duplicates-hypothesis/\"\n",
    "transfer_files = [\n",
    "    \"9-2020-transfers.parquet\",\n",
    "    \"10-2020-transfers.parquet\",\n",
    "    \"11-2020-transfers.parquet\",\n",
    "    \"12-2020-transfers.parquet\",\n",
    "    \"1-2021-transfers.parquet\",\n",
    "    \"2-2021-transfers.parquet\"\n",
    "]\n",
    "\n",
    "transfer_input_files = [transfer_file_location + f for f in transfer_files]\n",
    "transfers_raw = pd.concat((\n",
    "    pd.read_parquet(f)\n",
    "    for f in transfer_input_files\n",
    "))\n",
    "\n",
    "# In the data from the PRMT-1742-duplicates-analysis branch, these columns have been added , but contain only empty values.\n",
    "transfers_raw = transfers_raw.drop([\"sending_supplier\", \"requesting_supplier\"], axis=1)\n",
    "\n",
    "# Given the findings in PRMT-1742 - many duplicate EHR errors are misclassified, the below reclassifies the relevant data\n",
    "has_at_least_one_successful_integration_code = lambda errors: any((np.isnan(e) or e==15 for e in errors))\n",
    "successful_transfers_bool = transfers_raw['request_completed_ack_codes'].apply(has_at_least_one_successful_integration_code)\n",
    "transfers = transfers_raw.copy()\n",
    "transfers.loc[successful_transfers_bool, \"status\"] = \"INTEGRATED\"\n",
    "\n",
    "# Correctly interpret certain sender errors as failed.\n",
    "# This is explained in PRMT-1974. Eventually this will be fixed upstream in the pipeline.\n",
    "pending_sender_error_codes=[6,7,10,24,30,23,14,99]\n",
    "transfers_with_pending_sender_code_bool=transfers['sender_error_code'].isin(pending_sender_error_codes)\n",
    "transfers_with_pending_with_error_bool=transfers['status']=='PENDING_WITH_ERROR'\n",
    "transfers_which_need_pending_to_failure_change_bool=transfers_with_pending_sender_code_bool & transfers_with_pending_with_error_bool\n",
    "transfers.loc[transfers_which_need_pending_to_failure_change_bool,'status']='FAILED'\n",
    "\n",
    "# Add integrated Late status\n",
    "eight_days_in_seconds=8*24*60*60\n",
    "transfers_after_sla_bool=transfers['sla_duration']>eight_days_in_seconds\n",
    "transfers_with_integrated_bool=transfers['status']=='INTEGRATED'\n",
    "transfers_integrated_late_bool=transfers_after_sla_bool & transfers_with_integrated_bool\n",
    "transfers.loc[transfers_integrated_late_bool,'status']='INTEGRATED LATE'\n",
    "\n",
    "# If the record integrated after 28 days, change the status back to pending.\n",
    "# This is to handle each month consistently and to always reflect a transfers status 28 days after it was made.\n",
    "# TBD how this is handled upstream in the pipeline\n",
    "twenty_eight_days_in_seconds=28*24*60*60\n",
    "transfers_after_month_bool=transfers['sla_duration']>twenty_eight_days_in_seconds\n",
    "transfers_pending_at_month_bool=transfers_after_month_bool & transfers_integrated_late_bool\n",
    "transfers.loc[transfers_pending_at_month_bool,'status']='PENDING'\n",
    "transfers_with_early_error_bool=(~transfers.loc[:,'sender_error_code'].isna()) |(~transfers.loc[:,'intermediate_error_codes'].apply(len)>0)\n",
    "transfers.loc[transfers_with_early_error_bool & transfers_pending_at_month_bool,'status']='PENDING_WITH_ERROR'\n",
    "\n",
    "# Supplier name mapping\n",
    "supplier_renaming = {\n",
    "    \"EGTON MEDICAL INFORMATION SYSTEMS LTD (EMIS)\":\"EMIS\",\n",
    "    \"IN PRACTICE SYSTEMS LTD\":\"Vision\",\n",
    "    \"MICROTEST LTD\":\"Microtest\",\n",
    "    \"THE PHOENIX PARTNERSHIP\":\"TPP\",\n",
    "    None: \"Unknown\"\n",
    "}\n",
    "\n",
    "asid_lookup_file = \"s3://prm-gp2gp-data-sandbox-dev/asid-lookup/asidLookup-Mar-2021.csv.gz\"\n",
    "asid_lookup = pd.read_csv(asid_lookup_file)\n",
    "lookup = asid_lookup[[\"ASID\", \"MName\", \"NACS\",\"OrgName\"]]\n",
    "\n",
    "transfers = transfers.merge(lookup, left_on='requesting_practice_asid',right_on='ASID',how='left')\n",
    "transfers = transfers.rename({'MName': 'requesting_supplier', 'ASID': 'requesting_supplier_asid', 'NACS': 'requesting_ods_code','OrgName':'requesting_practice_name'}, axis=1)\n",
    "transfers = transfers.merge(lookup, left_on='sending_practice_asid',right_on='ASID',how='left')\n",
    "transfers = transfers.rename({'MName': 'sending_supplier', 'ASID': 'sending_supplier_asid', 'NACS': 'sending_ods_code','OrgName':'sending_practice_name'}, axis=1)\n",
    "\n",
    "transfers[\"sending_supplier\"] = transfers[\"sending_supplier\"].replace(supplier_renaming.keys(), supplier_renaming.values())\n",
    "transfers[\"requesting_supplier\"] = transfers[\"requesting_supplier\"].replace(supplier_renaming.keys(), supplier_renaming.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fdb06d",
   "metadata": {},
   "source": [
    "## Stage 1\n",
    "\n",
    "For each of the message from the Spine raw data, we extract the following:\n",
    "- supplier_type (creator of the message): \"sender\", \"requestor\"\n",
    "- messageType: interactionName (e.g. \"request completed\")\n",
    "- errorCode: None / int\n",
    "To produce for each message the following list: e.g. `[\"sender\", \"interactionName\", None]`\n",
    "\n",
    "Then for each transfer, we merged with the above to produce the following:\n",
    "- conversationID: str\n",
    "- messages: list of messages in the transfer e.g. `[[\"sender\", \"interactionName\", None], [\"sender\", \"interactionName\", None]]`\n",
    "\n",
    "We then save this as a parquet file to the following location: s3://prm-gp2gp-data-sandbox-dev/extra-fields-data-from-splunk/Sept_20_Feb_21_conversations_extended_interaction_messages.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26f27226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a mapping of practice asid, and whether they were the sender or requestor in that conversation\n",
    "requesting_supplier_type_map = transfers_raw[[\"conversation_id\", \"requesting_practice_asid\", \"date_requested\"]].drop_duplicates()\n",
    "sending_supplier_type_map = transfers_raw[[\"conversation_id\", \"sending_practice_asid\", \"date_requested\"]].drop_duplicates()\n",
    "\n",
    "requesting_supplier_type_map[\"supplier_type\"] = \"requestor\"\n",
    "sending_supplier_type_map[\"supplier_type\"] = \"sender\"\n",
    "\n",
    "requesting_supplier_type_map = requesting_supplier_type_map.rename({\"requesting_practice_asid\": \"practice_asid\"}, axis=1)\n",
    "sending_supplier_type_map = sending_supplier_type_map.rename({\"sending_practice_asid\": \"practice_asid\"}, axis=1)\n",
    "\n",
    "supplier_type_mapping = pd.concat([requesting_supplier_type_map, sending_supplier_type_map])\n",
    "supplier_type_mapping[\"practice_asid\"] = supplier_type_mapping[\"practice_asid\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdfdf23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of files to be loaded in\n",
    "folder=\"s3://prm-gp2gp-data-sandbox-dev/spine-gp2gp-data/\"\n",
    "files=[\"Sept-2020\",\"Oct-2020\", \"Nov-2020\",\"Dec-2020\",\"Jan-2021\",\"Feb-2021\",\"Mar-2021\"]\n",
    "full_filenames=[folder + file + \".csv.gz\" for file in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d214fdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename message types to be human readable\n",
    "interaction_name_mapping={\"urn:nhs:names:services:gp2gp/RCMR_IN010000UK05\":\"req start\",\n",
    "\"urn:nhs:names:services:gp2gp/RCMR_IN030000UK06\":\"req complete\",\n",
    "\"urn:nhs:names:services:gp2gp/COPC_IN000001UK01\":\"COPC\",\n",
    "\"urn:nhs:names:services:gp2gp/MCCI_IN010000UK13\":\"app ack\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57ac41e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_single_frame(file):\n",
    "    a=time.perf_counter()\n",
    "    print(\"Now Processing \" + file)\n",
    "    df=pd.read_csv(file, compression='gzip',error_bad_lines=False)\n",
    "    df=df.sort_values(by='_time')\n",
    "\n",
    "    # filter out conversations that are not relevant transfers & maps whether supplier who sent message is the requesting or sending practice\n",
    "    df = df.merge(supplier_type_mapping, left_on=[\"conversationID\", \"messageSender\"], right_on=[\"conversation_id\", \"practice_asid\"], how=\"left\")\n",
    "    \n",
    "    # filter out messages that took place more than 28 days after the date requested\n",
    "    in_time_message_bool = (pd.to_datetime(df[\"_time\"]).dt.tz_localize(None) - df[\"date_requested\"]).dt.seconds <= twenty_eight_days_in_seconds\n",
    "    df = df.loc[in_time_message_bool]\n",
    "    \n",
    "    # map the message name to human readable form using supplier mapping\n",
    "    df['interaction_name']=df['interactionID'].replace(interaction_name_mapping)\n",
    "    df[\"jdiEvent\"] = df[\"jdiEvent\"].replace(\"NONE\", \"\")\n",
    "    \n",
    "    # Create a message component for each message - that is a list (containing supplier type, interaction name and error codes)\n",
    "    df[\"messages\"] = list(zip(df[\"supplier_type\"], df[\"interaction_name\"], df[\"jdiEvent\"]))\n",
    "    df[\"messages\"] = df[\"messages\"].apply(list)\n",
    "    df=df[[\"conversation_id\", \"messages\"]]\n",
    "    print(time.perf_counter()-a)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46e1e3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now Processing s3://prm-gp2gp-data-sandbox-dev/spine-gp2gp-data/Sept-2020.csv.gz\n",
      "67.18952013700073\n",
      "Now Processing s3://prm-gp2gp-data-sandbox-dev/spine-gp2gp-data/Oct-2020.csv.gz\n",
      "59.009422072000234\n",
      "Now Processing s3://prm-gp2gp-data-sandbox-dev/spine-gp2gp-data/Nov-2020.csv.gz\n",
      "53.919303392000074\n",
      "Now Processing s3://prm-gp2gp-data-sandbox-dev/spine-gp2gp-data/Dec-2020.csv.gz\n",
      "49.77571221399921\n",
      "Now Processing s3://prm-gp2gp-data-sandbox-dev/spine-gp2gp-data/Jan-2021.csv.gz\n",
      "56.439507076000154\n",
      "Now Processing s3://prm-gp2gp-data-sandbox-dev/spine-gp2gp-data/Feb-2021.csv.gz\n",
      "58.64482386600139\n",
      "Now Processing s3://prm-gp2gp-data-sandbox-dev/spine-gp2gp-data/Mar-2021.csv.gz\n",
      "61.371659021000596\n"
     ]
    }
   ],
   "source": [
    "field_data=[generate_single_frame(file) for file in full_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70aad70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now Concatenating all months of data\n",
      "1.0856060549995163\n",
      "Now identifying and removing NaN conversation IDs\n",
      "3.189734428999145\n",
      "Now Grouping by conversation\n",
      "69.0288721149991\n"
     ]
    }
   ],
   "source": [
    "print('Now Concatenating all months of data')\n",
    "a=time.perf_counter()\n",
    "full_field_data=pd.concat(field_data,axis=0)\n",
    "print(time.perf_counter()-a)\n",
    "\n",
    "print('Now identifying and removing NaN conversation IDs')\n",
    "a=time.perf_counter()\n",
    "valid_conversation_bool=~full_field_data['conversation_id'].isna()\n",
    "full_field_data=full_field_data.loc[valid_conversation_bool]\n",
    "print(time.perf_counter()-a)\n",
    "\n",
    "print('Now Grouping by conversation')\n",
    "a=time.perf_counter()\n",
    "full_field_data=full_field_data.groupby('conversation_id')['messages'].apply(list)\n",
    "print(time.perf_counter()-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40c0e6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now Saving Data\n"
     ]
    }
   ],
   "source": [
    "print('Now Saving Data')\n",
    "\n",
    "if overwrite_files:\n",
    "    pd.DataFrame(full_field_data).to_parquet('s3://prm-gp2gp-data-sandbox-dev/extra-fields-data-from-splunk/Sept_20_Feb_21_conversations_extended_interaction_messages.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d99ac3",
   "metadata": {},
   "source": [
    "## Stage 2.\n",
    "\n",
    "a. We load in the original transfer data and for each transfer, and for each transfer we will have a list all messages associated (by joining in the message list dataset from above)\n",
    "\n",
    "b. We group by supplier pathway, status and message\n",
    "     - We count for each one, order it, add percentages and this is the final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7185397d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations_extended_interaction_messages=pd.read_parquet('s3://prm-gp2gp-data-sandbox-dev/extra-fields-data-from-splunk/Sept_20_Feb_21_conversations_extended_interaction_messages.parquet')\n",
    "# turning messages from list of list to tuple of tuples (since they are hasable)\n",
    "conversations_extended_interaction_messages[\"messages\"]=conversations_extended_interaction_messages[\"messages\"].apply(lambda message_list: tuple([tuple(message) for message in message_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae1281e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfers_with_message_list = transfers.merge(conversations_extended_interaction_messages, left_on=\"conversation_id\", right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee7613db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of transfers per status and supplier pathway and message pattern combination\n",
    "message_list_prevelance_table = transfers_with_message_list.groupby([\"sending_supplier\", \"requesting_supplier\", \"status\", \"messages\"]).agg({\"conversation_id\": \"count\"})\n",
    "message_list_prevelance_table = message_list_prevelance_table.rename({\"conversation_id\": \"Total Number of transfers\"}, axis=1).sort_values(by=\"Total Number of transfers\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f693007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_transfer_count = message_list_prevelance_table[\"Total Number of transfers\"].sum()\n",
    "message_list_prevelance_table[\"% Transfers\"] = (message_list_prevelance_table[\"Total Number of transfers\"] / total_transfer_count).multiply(100).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "722fcdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's add a column with the percentage of transfers for that combination of Supplier Pathway and status\n",
    "\n",
    "# The columns we are aggregating on (ie supplier pathway and status)\n",
    "column_interested_in = [\"sending_supplier\", \"requesting_supplier\", \"status\"]\n",
    "\n",
    "# Create a table of the count of transfers for each of these supplier pathways\n",
    "pathway_and_status_counts = transfers_with_message_list.groupby(column_interested_in).agg({\"conversation_id\": \"count\"})\n",
    "pathway_and_status_counts = pathway_and_status_counts.rename({\"conversation_id\": \"Pathway and status totals\"}, axis=1)\n",
    "\n",
    "# Take the relevant indexes from our original table and use this to get a full list of the number of transfers for each row's pathway and status\n",
    "order_of_indexes_needed=message_list_prevelance_table.reset_index().set_index(column_interested_in).index\n",
    "ordered_totals=pathway_and_status_counts.loc[order_of_indexes_needed]\n",
    "\n",
    "# Divide the transfers by these values to get the percentage\n",
    "message_list_prevelance_table[\"% Pathway and status transfers\"] =(message_list_prevelance_table['Total Number of transfers'].values/ordered_totals['Pathway and status totals'].values)\n",
    "message_list_prevelance_table[\"% Pathway and status transfers\"] =message_list_prevelance_table[\"% Pathway and status transfers\"].multiply(100).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7932fc41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(244114, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_list_prevelance_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "996828f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12418, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering out the combinations that only have one transfer associated\n",
    "reduced_message_list_prevelance_table_bool = message_list_prevelance_table[\"Total Number of transfers\"] > 1\n",
    "reduced_message_list_prevelance_table = message_list_prevelance_table[reduced_message_list_prevelance_table_bool]\n",
    "reduced_message_list_prevelance_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ada8376f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: there are 231696 combinations of supplier pathway, status and message pattern with only 1 transfer associated, which we have filtered out\n"
     ]
    }
   ],
   "source": [
    "print(f\"Note: there are {message_list_prevelance_table.shape[0] - reduced_message_list_prevelance_table.shape[0]} combinations of supplier pathway, status and message pattern with only 1 transfer associated, which we have filtered out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46801b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "if overwrite_files:\n",
    "    pd.DataFrame(reduced_message_list_prevelance_table).to_csv('s3://prm-gp2gp-data-sandbox-dev/notebook-outputs/36--PRMT-2059-high-level-table-of-message-patterns-reduced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98df72a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
