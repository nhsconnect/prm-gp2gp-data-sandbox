{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a6b6a41",
   "metadata": {},
   "source": [
    "# PRMT-2040 Investigate EMIS placeholder data in MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfab4aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4b792b",
   "metadata": {},
   "source": [
    "# Import Transfers Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5cd0290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import transfer files to extract whether message creator is sender or requester\n",
    "# Using data generated from branch PRMT-1742-duplicates-analysis.\n",
    "# This is needed to correctly handle duplicates.\n",
    "# Once the upstream pipeline has a fix for duplicate EHRs, then we can go back to using the main output.\n",
    "transfer_file_location = \"s3://prm-gp2gp-data-sandbox-dev/transfers-duplicates-hypothesis/\"\n",
    "transfer_files = [\n",
    "    \"9-2020-transfers.parquet\",\n",
    "    \"10-2020-transfers.parquet\",\n",
    "    \"11-2020-transfers.parquet\",\n",
    "    \"12-2020-transfers.parquet\",\n",
    "    \"1-2021-transfers.parquet\",\n",
    "    \"2-2021-transfers.parquet\"\n",
    "]\n",
    "\n",
    "transfer_input_files = [transfer_file_location + f for f in transfer_files]\n",
    "transfers_raw = pd.concat((\n",
    "    pd.read_parquet(f)\n",
    "    for f in transfer_input_files\n",
    "))\n",
    "\n",
    "# In the data from the PRMT-1742-duplicates-analysis branch, these columns have been added , but contain only empty values.\n",
    "transfers_raw = transfers_raw.drop([\"sending_supplier\", \"requesting_supplier\"], axis=1)\n",
    "\n",
    "# Given the findings in PRMT-1742 - many duplicate EHR errors are misclassified, the below reclassifies the relevant data\n",
    "has_at_least_one_successful_integration_code = lambda errors: any((np.isnan(e) or e==15 for e in errors))\n",
    "successful_transfers_bool = transfers_raw['request_completed_ack_codes'].apply(has_at_least_one_successful_integration_code)\n",
    "transfers = transfers_raw.copy()\n",
    "transfers.loc[successful_transfers_bool, \"status\"] = \"INTEGRATED\"\n",
    "\n",
    "# Correctly interpret certain sender errors as failed.\n",
    "# This is explained in PRMT-1974. Eventually this will be fixed upstream in the pipeline.\n",
    "pending_sender_error_codes=[6,7,10,24,30,23,14,99]\n",
    "transfers_with_pending_sender_code_bool=transfers['sender_error_code'].isin(pending_sender_error_codes)\n",
    "transfers_with_pending_with_error_bool=transfers['status']=='PENDING_WITH_ERROR'\n",
    "transfers_which_need_pending_to_failure_change_bool=transfers_with_pending_sender_code_bool & transfers_with_pending_with_error_bool\n",
    "transfers.loc[transfers_which_need_pending_to_failure_change_bool,'status']='FAILED'\n",
    "\n",
    "# Add integrated Late status\n",
    "eight_days_in_seconds=8*24*60*60\n",
    "transfers_after_sla_bool=transfers['sla_duration']>eight_days_in_seconds\n",
    "transfers_with_integrated_bool=transfers['status']=='INTEGRATED'\n",
    "transfers_integrated_late_bool=transfers_after_sla_bool & transfers_with_integrated_bool\n",
    "transfers.loc[transfers_integrated_late_bool,'status']='INTEGRATED LATE'\n",
    "\n",
    "# If the record integrated after 28 days, change the status back to pending.\n",
    "# This is to handle each month consistently and to always reflect a transfers status 28 days after it was made.\n",
    "# TBD how this is handled upstream in the pipeline\n",
    "twenty_eight_days_in_seconds=28*24*60*60\n",
    "transfers_after_month_bool=transfers['sla_duration']>twenty_eight_days_in_seconds\n",
    "transfers_pending_at_month_bool=transfers_after_month_bool & transfers_integrated_late_bool\n",
    "transfers.loc[transfers_pending_at_month_bool,'status']='PENDING'\n",
    "transfers_with_early_error_bool=(~transfers.loc[:,'sender_error_code'].isna()) |(~transfers.loc[:,'intermediate_error_codes'].apply(len)>0)\n",
    "transfers.loc[transfers_with_early_error_bool & transfers_pending_at_month_bool,'status']='PENDING_WITH_ERROR'\n",
    "\n",
    "# Supplier name mapping\n",
    "supplier_renaming = {\n",
    "    \"EGTON MEDICAL INFORMATION SYSTEMS LTD (EMIS)\":\"EMIS\",\n",
    "    \"IN PRACTICE SYSTEMS LTD\":\"Vision\",\n",
    "    \"MICROTEST LTD\":\"Microtest\",\n",
    "    \"THE PHOENIX PARTNERSHIP\":\"TPP\",\n",
    "    None: \"Unknown\"\n",
    "}\n",
    "\n",
    "asid_lookup_file = \"s3://prm-gp2gp-data-sandbox-dev/asid-lookup/asidLookup-Mar-2021.csv.gz\"\n",
    "asid_lookup = pd.read_csv(asid_lookup_file)\n",
    "lookup = asid_lookup[[\"ASID\", \"MName\", \"NACS\",\"OrgName\"]]\n",
    "\n",
    "transfers = transfers.merge(lookup, left_on='requesting_practice_asid',right_on='ASID',how='left')\n",
    "transfers = transfers.rename({'MName': 'requesting_supplier', 'ASID': 'requesting_supplier_asid', 'NACS': 'requesting_ods_code','OrgName':'requesting_practice_name'}, axis=1)\n",
    "transfers = transfers.merge(lookup, left_on='sending_practice_asid',right_on='ASID',how='left')\n",
    "transfers = transfers.rename({'MName': 'sending_supplier', 'ASID': 'sending_supplier_asid', 'NACS': 'sending_ods_code','OrgName':'sending_practice_name'}, axis=1)\n",
    "\n",
    "transfers[\"sending_supplier\"] = transfers[\"sending_supplier\"].replace(supplier_renaming.keys(), supplier_renaming.values())\n",
    "transfers[\"requesting_supplier\"] = transfers[\"requesting_supplier\"].replace(supplier_renaming.keys(), supplier_renaming.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f837bb9",
   "metadata": {},
   "source": [
    "# Import MI SR Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffbce02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3072: DtypeWarning: Columns (11,16) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1502251, 34)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MI_Data=pd.read_csv(\"s3://prm-gp2gp-data-sandbox-dev/MI_athena_outputs/MI_SR_Sept_20_Feb_21.csv\")\n",
    "MI_Data['conversation_id']=MI_Data['ConversationID'].str.upper()\n",
    "MI_Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbd27cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of MI conversations from Athena: 1310479\n",
      "Number of transfer conversations from Spine: 1343234\n",
      "Number of overlapping conversations between Athena and Spine: 1309920\n",
      "How many conversations are present in Athena MI but missing from Spine Data: 559\n",
      "Senders of conversations which are present in Spine Data but missing from Athena MI\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sending_supplier</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EMIS</th>\n",
       "      <td>1320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Microtest</th>\n",
       "      <td>1029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TPP</th>\n",
       "      <td>386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unknown</th>\n",
       "      <td>1487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vision</th>\n",
       "      <td>29092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  conversation_id\n",
       "sending_supplier                 \n",
       "EMIS                         1320\n",
       "Microtest                    1029\n",
       "TPP                           386\n",
       "Unknown                      1487\n",
       "Vision                      29092"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MI_conversations=set(MI_Data['conversation_id'].values)\n",
    "spine_conversations=set(transfers['conversation_id'].values)\n",
    "overlap_conversations=MI_conversations.intersection(spine_conversations)\n",
    "print(f'Number of MI conversations from Athena: {len(MI_conversations)}')\n",
    "print(f'Number of transfer conversations from Spine: {len(spine_conversations)}')\n",
    "print(f'Number of overlapping conversations between Athena and Spine: {len(overlap_conversations)}')\n",
    "print(f'How many conversations are present in Athena MI but missing from Spine Data: {len(MI_conversations)-len(overlap_conversations)}')\n",
    "\n",
    "print('Senders of conversations which are present in Spine Data but missing from Athena MI')\n",
    "non_mi_conversations_bool=~transfers['conversation_id'].isin(MI_conversations)\n",
    "transfers.loc[non_mi_conversations_bool].groupby(['sending_supplier']).agg({'conversation_id':'count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bea763c",
   "metadata": {},
   "source": [
    "## Does the Athena MI pull match an equivalent pull from Splunk?\n",
    "We found a discrepency between Splunk and Athena. Splunk was 1,045,674 results, while Athena was 1,502,251.\n",
    "We investigate why below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a45357c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Splunk source files: 8888\n",
      "Number of Athena source files: 55249\n",
      "Intersection of Files: 8888\n"
     ]
    }
   ],
   "source": [
    "# Investigation of whether all the same source files were used\n",
    "file_names_athena = pd.read_csv(\"file-names-athena.csv\")\n",
    "file_names_splunk = pd.read_csv(\"file-names.csv\")\n",
    "print(f'Number of Splunk source files: {file_names_splunk.shape[0]}')\n",
    "print(f'Number of Athena source files: {file_names_athena.shape[0]}')\n",
    "file_names_splunk_set = set(file_names_splunk[\"source\"].values)\n",
    "file_names_athena_set = set(file_names_athena[\"path\"].values)\n",
    "print(f'Intersection of Files: {len(file_names_athena_set.intersection(file_names_splunk_set))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70e62333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Contains Whitespace</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Is in Splunk</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>160</td>\n",
       "      <td>46201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>8888</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Contains Whitespace  False  True \n",
       "Is in Splunk                     \n",
       "False                  160  46201\n",
       "True                  8888      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_frame=file_names_athena.copy()\n",
    "comparison_frame['Is in Splunk']=comparison_frame['path'].isin(file_names_splunk_set)\n",
    "comparison_frame['Contains Whitespace']=comparison_frame['path'].str.contains(\" \")\n",
    "comparison_frame.pivot_table(index='Is in Splunk',columns='Contains Whitespace',aggfunc='count',values='_col1').fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56580f8b",
   "metadata": {},
   "source": [
    "Further investigation of the same queries but just for 1st March to 1st May showed they both produced the same number of events (ie rows).\n",
    "So the fix that took place in February (replacing whitespaces in filenames with underscores) has corrected this issue but has not been applied retroactively. \n",
    "The 160 files without whitespaces remain a mystery and are listed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9e714db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1545     s3://prm-gp2gp-mi-data-prod-v2/2020/12/06/2eba...\n",
       "1973     s3://prm-gp2gp-mi-data-prod-v2/2021/01/03/9343...\n",
       "2052     s3://prm-gp2gp-mi-data-prod-v2/2020/12/06/c90f...\n",
       "2408     s3://prm-gp2gp-mi-data-prod-v2/2020/12/20/779a...\n",
       "2683     s3://prm-gp2gp-mi-data-prod-v2/2020/12/27/9ff4...\n",
       "                               ...                        \n",
       "53424    s3://prm-gp2gp-mi-data-prod-v2/2021/01/03/8e83...\n",
       "53542    s3://prm-gp2gp-mi-data-prod-v2/2020/12/13/43a7...\n",
       "53956    s3://prm-gp2gp-mi-data-prod-v2/2021/01/03/f0d6...\n",
       "54181    s3://prm-gp2gp-mi-data-prod-v2/2021/01/03/15bb...\n",
       "55173    s3://prm-gp2gp-mi-data-prod-v2/2021/01/10/e314...\n",
       "Name: path, Length: 160, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_in_splunk_no_whitespace_bool=(~comparison_frame['Is in Splunk']) & (~comparison_frame['Contains Whitespace'])\n",
    "comparison_frame.loc[not_in_splunk_no_whitespace_bool,\"path\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501e46be",
   "metadata": {},
   "source": [
    "## Does the Athena MI Data have repeating conversations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d0191fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     1198815\n",
       "2      109746\n",
       "3        1786\n",
       "4          85\n",
       "5          26\n",
       "6           9\n",
       "7           5\n",
       "8           3\n",
       "10          2\n",
       "9           2\n",
       "Name: conversation_id, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How often do conversations repeat but with different values?\n",
    "MI_Data.drop_duplicates()['conversation_id'].value_counts().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0e35e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1310418\n",
       "2         61\n",
       "Name: conversation_id, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For just the placeholder data, how often do conversations repeat with different placeholder data?\n",
    "placeholder_columns=['PlaceholdersFileTypeUnsupported', 'PlaceholdersFileDeleted',\n",
    "       'PlaceholdersFileNotFound', 'PlaceholdersFileLocked',\n",
    "       'PlaceholdersUndeterminedReason']\n",
    "data_of_interest=MI_Data.loc[:,placeholder_columns +['conversation_id']].drop_duplicates()\n",
    "data_of_interest['conversation_id'].value_counts().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d202e242",
   "metadata": {},
   "source": [
    "# Parse out EMIS placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "278d5caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PlaceholdersFileTypeUnsupported</th>\n",
       "      <th>PlaceholdersFileDeleted</th>\n",
       "      <th>PlaceholdersFileNotFound</th>\n",
       "      <th>PlaceholdersFileLocked</th>\n",
       "      <th>PlaceholdersUndeterminedReason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>86.97</td>\n",
       "      <td>100.0</td>\n",
       "      <td>97.32</td>\n",
       "      <td>100.0</td>\n",
       "      <td>98.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>2.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>1.32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>1.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.0</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.0</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.0</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PlaceholdersFileTypeUnsupported  PlaceholdersFileDeleted  \\\n",
       "0.0                            86.97                    100.0   \n",
       "1.0                             2.10                      0.0   \n",
       "2.0                             1.32                      0.0   \n",
       "3.0                             1.01                      0.0   \n",
       "4.0                             0.78                      0.0   \n",
       "5.0                             0.68                      0.0   \n",
       "6.0                             0.58                      0.0   \n",
       "7.0                             0.50                      0.0   \n",
       "8.0                             0.45                      0.0   \n",
       "9.0                             0.39                      0.0   \n",
       "\n",
       "     PlaceholdersFileNotFound  PlaceholdersFileLocked  \\\n",
       "0.0                     97.32                   100.0   \n",
       "1.0                      1.36                     0.0   \n",
       "2.0                      0.49                     0.0   \n",
       "3.0                      0.24                     0.0   \n",
       "4.0                      0.15                     0.0   \n",
       "5.0                      0.09                     0.0   \n",
       "6.0                      0.06                     0.0   \n",
       "7.0                      0.04                     0.0   \n",
       "8.0                      0.03                     0.0   \n",
       "9.0                      0.02                     0.0   \n",
       "\n",
       "     PlaceholdersUndeterminedReason  \n",
       "0.0                           98.55  \n",
       "1.0                            0.73  \n",
       "2.0                            0.25  \n",
       "3.0                            0.12  \n",
       "4.0                            0.08  \n",
       "5.0                            0.05  \n",
       "6.0                            0.04  \n",
       "7.0                            0.03  \n",
       "8.0                            0.02  \n",
       "9.0                            0.02  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "placeholder_data=MI_Data.loc[:,placeholder_columns +['conversation_id']].drop_duplicates()\n",
    "\n",
    "EMIS_sender_conversation_ids=transfers.loc[transfers['sending_supplier']=='EMIS','conversation_id'].values\n",
    "\n",
    "EMIS_sender_MI_bool=placeholder_data['conversation_id'].isin(EMIS_sender_conversation_ids)\n",
    "EMIS_sender_MI_data=placeholder_data[EMIS_sender_MI_bool]\n",
    "\n",
    "quantity_EMIS_transfers=EMIS_sender_MI_data.shape[0]\n",
    "\n",
    "pd.concat([pd.DataFrame(EMIS_sender_MI_data[pc_column].value_counts(dropna=False)) for pc_column in placeholder_columns],axis=1).fillna(0).div(quantity_EMIS_transfers).multiply(100).round(2).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87627182",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
