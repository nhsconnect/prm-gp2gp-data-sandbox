{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f384301",
   "metadata": {},
   "source": [
    "# PRMT-2124 - Can we consider transfers without a Sender Request Completed message as failed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcf47c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63e62db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import transfer files to extract whether message creator is sender or requester\n",
    "transfer_file_location = \"s3://prm-gp2gp-data-sandbox-dev/transfers-sample-5/\"\n",
    "transfer_files = [\n",
    "    \"2020-9-transfers.parquet\",\n",
    "    \"2020-10-transfers.parquet\",\n",
    "    \"2020-11-transfers.parquet\",\n",
    "    \"2020-12-transfers.parquet\",\n",
    "    \"2021-1-transfers.parquet\",\n",
    "    \"2021-2-transfers.parquet\",\n",
    "    \"2021-3-transfers.parquet\"\n",
    "]\n",
    "\n",
    "transfer_input_files = [transfer_file_location + f for f in transfer_files]\n",
    "transfers_raw = pd.concat((\n",
    "    pd.read_parquet(f)\n",
    "    for f in transfer_input_files\n",
    "))\n",
    "\n",
    "# In the data from the PRMT-1742-duplicates-analysis branch, these columns have been added , but contain only empty values.\n",
    "transfers_raw = transfers_raw.drop([\"sending_supplier\", \"requesting_supplier\"], axis=1)\n",
    "transfers = transfers_raw.copy()\n",
    "\n",
    "# Correctly interpret certain sender errors as failed.\n",
    "# This is explained in PRMT-1974. Eventually this will be fixed upstream in the pipeline.\n",
    "# Step Two: reclassifying the relevant transfers with pending sender error codes to FAILED DUE TO SENDER ERROR CODE status for comparison\n",
    "pending_sender_error_codes=[6,7,10,24,30,23,14,99]\n",
    "transfers_with_pending_sender_code_bool=transfers['sender_error_code'].isin(pending_sender_error_codes)\n",
    "transfers_with_pending_with_error_bool=transfers['status']=='PENDING_WITH_ERROR'\n",
    "transfers_which_need_pending_to_failure_change_bool=transfers_with_pending_sender_code_bool & transfers_with_pending_with_error_bool\n",
    "transfers.loc[transfers_which_need_pending_to_failure_change_bool,'status']='FAILED DUE TO SENDER ERROR CODE'\n",
    "\n",
    "# Add integrated Late status\n",
    "eight_days_in_seconds=8*24*60*60\n",
    "transfers_after_sla_bool=transfers['sla_duration']>eight_days_in_seconds\n",
    "transfers_with_integrated_bool=transfers['status']=='INTEGRATED'\n",
    "transfers_integrated_late_bool=transfers_after_sla_bool & transfers_with_integrated_bool\n",
    "transfers.loc[transfers_integrated_late_bool,'status']='INTEGRATED LATE'\n",
    "\n",
    "# If the record integrated after 28 days, change the status back to pending.\n",
    "# This is to handle each month consistently and to always reflect a transfers status 28 days after it was made.\n",
    "# TBD how this is handled upstream in the pipeline\n",
    "twenty_eight_days_in_seconds=28*24*60*60\n",
    "transfers_after_month_bool=transfers['sla_duration']>twenty_eight_days_in_seconds\n",
    "transfers_pending_at_month_bool=transfers_after_month_bool & transfers_integrated_late_bool\n",
    "transfers.loc[transfers_pending_at_month_bool,'status']='PENDING'\n",
    "transfers_with_early_error_bool=(~transfers.loc[:,'sender_error_code'].isna()) |(~transfers.loc[:,'intermediate_error_codes'].apply(len)>0)\n",
    "transfers.loc[transfers_with_early_error_bool & transfers_pending_at_month_bool,'status']='PENDING_WITH_ERROR'\n",
    "\n",
    "# Supplier name mapping\n",
    "supplier_renaming = {\n",
    "    \"EGTON MEDICAL INFORMATION SYSTEMS LTD (EMIS)\":\"EMIS\",\n",
    "    \"IN PRACTICE SYSTEMS LTD\":\"Vision\",\n",
    "    \"MICROTEST LTD\":\"Microtest\",\n",
    "    \"THE PHOENIX PARTNERSHIP\":\"TPP\",\n",
    "    None: \"Unknown\"\n",
    "}\n",
    "\n",
    "# Generate ASID lookup that contains all the most recent entry for all ASIDs encountered\n",
    "asid_file_location = \"s3://prm-gp2gp-data-sandbox-dev/asid-lookup/\"\n",
    "asid_files = [\n",
    "    \"asidLookup-Nov-2020.csv.gz\",\n",
    "    \"asidLookup-Dec-2020.csv.gz\",\n",
    "    \"asidLookup-Jan-2021.csv.gz\",\n",
    "    \"asidLookup-Feb-2021.csv.gz\",\n",
    "    \"asidLookup-Mar-2021.csv.gz\"\n",
    "]\n",
    "asid_lookup_files = [asid_file_location + f for f in asid_files]\n",
    "asid_lookup = pd.concat((\n",
    "    pd.read_csv(f)\n",
    "    for f in asid_lookup_files\n",
    "))\n",
    "asid_lookup = asid_lookup.drop_duplicates().groupby(\"ASID\").last().reset_index()\n",
    "lookup = asid_lookup[[\"ASID\", \"MName\", \"NACS\",\"OrgName\"]]\n",
    "\n",
    "transfers = transfers.merge(lookup, left_on='requesting_practice_asid',right_on='ASID',how='left')\n",
    "transfers = transfers.rename({'MName': 'requesting_supplier', 'ASID': 'requesting_supplier_asid', 'NACS': 'requesting_ods_code','OrgName':'requesting_practice_name'}, axis=1)\n",
    "transfers = transfers.merge(lookup, left_on='sending_practice_asid',right_on='ASID',how='left')\n",
    "transfers = transfers.rename({'MName': 'sending_supplier', 'ASID': 'sending_supplier_asid', 'NACS': 'sending_ods_code','OrgName':'sending_practice_name'}, axis=1)\n",
    "\n",
    "transfers[\"sending_supplier\"] = transfers[\"sending_supplier\"].replace(supplier_renaming.keys(), supplier_renaming.values())\n",
    "transfers[\"requesting_supplier\"] = transfers[\"requesting_supplier\"].replace(supplier_renaming.keys(), supplier_renaming.values())\n",
    "\n",
    "# Making the status to be more human readable here\n",
    "transfers[\"status\"] = transfers[\"status\"].str.replace(\"_\", \" \").str.title()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "434ddcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_messages_parquet_to_transfers(parquet_file_name, transfers_df):\n",
    "    conversations_extended_interaction_messages=pd.read_parquet(parquet_file_name)\n",
    "    # turning messages from list of list to tuple of tuples (since they are hasable)\n",
    "    conversations_extended_interaction_messages[\"messages\"]=conversations_extended_interaction_messages[\"messages\"].apply(lambda message_list: tuple([tuple(message) for message in message_list]))\n",
    "    # Attach this message list to the transfers dataframe\n",
    "    transfers_with_message_list = transfers_df.merge(conversations_extended_interaction_messages, left_on=\"conversation_id\", right_index=True)\n",
    "    return transfers_with_message_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f181a3",
   "metadata": {},
   "source": [
    "## Part A\n",
    "\n",
    "**We believe that** transfers which do not receive a “Request Completed” message from the Sender within 14 days of the request started\n",
    "\n",
    "**can be considered** technical failures\n",
    "\n",
    "**we will know this to be true when** we see that for a sample of data without this message in 14 days, the message does not arrive, nor does transfer completion occur, within 1-8 months\n",
    "\n",
    "### Scope\n",
    "\n",
    "1. Extract all conversation ids from September to March (7 months) parquet files\n",
    "2. Identify all messages within these conversation IDs using raw Spine data from September to April\n",
    "3. Extract conversations with no Request Completed Message in the first 14 days\n",
    "4. Count how many of these contain a later Request Completed or successful transfer message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a767b12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite_files = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a46cdd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a mapping of practice asid, and whether they were the sender or requestor in that conversation\n",
    "requesting_supplier_type_map = transfers[[\"conversation_id\", \"requesting_practice_asid\", \"date_requested\"]].drop_duplicates()\n",
    "sending_supplier_type_map = transfers[[\"conversation_id\", \"sending_practice_asid\", \"date_requested\"]].drop_duplicates()\n",
    "\n",
    "requesting_supplier_type_map[\"supplier_type\"] = \"requestor\"\n",
    "sending_supplier_type_map[\"supplier_type\"] = \"sender\"\n",
    "\n",
    "requesting_supplier_type_map = requesting_supplier_type_map.rename({\"requesting_practice_asid\": \"practice_asid\"}, axis=1)\n",
    "sending_supplier_type_map = sending_supplier_type_map.rename({\"sending_practice_asid\": \"practice_asid\"}, axis=1)\n",
    "\n",
    "supplier_type_mapping = pd.concat([requesting_supplier_type_map, sending_supplier_type_map])\n",
    "supplier_type_mapping[\"practice_asid\"] = supplier_type_mapping[\"practice_asid\"].astype(int)\n",
    "\n",
    "conversation_ids_of_interest=transfers['conversation_id'].values\n",
    "\n",
    "# Define a list of files to be loaded in\n",
    "#folder=\"s3://prm-gp2gp-data-sandbox-dev/spine-gp2gp-data-with-ack-codes-prmt-2059/\"\n",
    "folder=\"s3://prm-gp2gp-data-sandbox-dev/spine-gp2gp-data/\"\n",
    "files=[\"Sept-2020\",\"Oct-2020\",\"Nov-2020\",\"Dec-2020\",\"Jan-2021\",\"Feb-2021\",\"Mar-2021\"]\n",
    "full_filenames=[folder + file + \".csv.gz\" for file in files]\n",
    "\n",
    "# Rename message types to be human readable\n",
    "interaction_name_mapping={\"urn:nhs:names:services:gp2gp/RCMR_IN010000UK05\":\"req start\",\n",
    "\"urn:nhs:names:services:gp2gp/RCMR_IN030000UK06\":\"req complete\",\n",
    "\"urn:nhs:names:services:gp2gp/COPC_IN000001UK01\":\"COPC\",\n",
    "\"urn:nhs:names:services:gp2gp/MCCI_IN010000UK13\":\" ack\"}\n",
    "\n",
    "#ackTypeCode_mapping={'AE':\"Neg\",\"AR\":\"Neg\",\"ER\":\"Neg\",\"IF\":\"Pos\",\"NONE\":\"Pos\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63f486f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will take a set of Spine data and for each message in the conversation we're interested in, it will output a dataframe\n",
    "# with \"conversation_id\", \"supplier_type\", \"interaction_name\", \"jdiEvent\", \"GUID\", \"messageRef\" for each given message in the order\n",
    "# they occur\n",
    "def generate_single_frame(file):\n",
    "    a=time.perf_counter()\n",
    "    print(\"Now Processing \" + file)\n",
    "    all_messages_in_file=pd.read_csv(file, compression='gzip')\n",
    "\n",
    "    # Only keep conversations from the conversations that we actually want to use\n",
    "    monthly_relevant_messages=all_messages_in_file.loc[all_messages_in_file['conversationID'].isin(conversation_ids_of_interest)]\n",
    "    monthly_relevant_messages=monthly_relevant_messages.sort_values(by='_time')\n",
    "    monthly_relevant_messages = monthly_relevant_messages.merge(supplier_type_mapping, left_on=[\"conversationID\", \"messageSender\"], right_on=[\"conversation_id\", \"practice_asid\"], how=\"left\")\n",
    "    \n",
    "    # Add time of messages occurring\n",
    "    monthly_relevant_messages[\"time_of_message\"] = (pd.to_datetime(monthly_relevant_messages[\"_time\"]).dt.tz_localize(None) - monthly_relevant_messages[\"date_requested\"]).dt.total_seconds().astype(str)\n",
    "    monthly_messages = monthly_relevant_messages\n",
    "\n",
    "    # map the message name to human readable form using supplier mapping\n",
    "    monthly_messages['interaction_name']=monthly_messages['interactionID'].replace(interaction_name_mapping)\n",
    "    monthly_messages[\"jdiEvent\"] = monthly_messages[\"jdiEvent\"].replace(\"NONE\", \"\")\n",
    "    \n",
    "    monthly_messages=monthly_messages[[\"conversation_id\",\"supplier_type\",\"interaction_name\",\"jdiEvent\",\"GUID\",\"messageRef\",\"time_of_message\" ]]\n",
    "    print(time.perf_counter()-a)\n",
    "    return monthly_messages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43930878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now Processing s3://prm-gp2gp-data-sandbox-dev/spine-gp2gp-data/Sept-2020.csv.gz\n",
      "59.076444541000456\n",
      "Now Processing s3://prm-gp2gp-data-sandbox-dev/spine-gp2gp-data/Oct-2020.csv.gz\n",
      "52.314487456998904\n",
      "Now Processing s3://prm-gp2gp-data-sandbox-dev/spine-gp2gp-data/Nov-2020.csv.gz\n",
      "48.13411116899988\n",
      "Now Processing s3://prm-gp2gp-data-sandbox-dev/spine-gp2gp-data/Dec-2020.csv.gz\n",
      "44.745309570000245\n",
      "Now Processing s3://prm-gp2gp-data-sandbox-dev/spine-gp2gp-data/Jan-2021.csv.gz\n",
      "50.18377329900068\n",
      "Now Processing s3://prm-gp2gp-data-sandbox-dev/spine-gp2gp-data/Feb-2021.csv.gz\n",
      "52.99704719299916\n",
      "Now Processing s3://prm-gp2gp-data-sandbox-dev/spine-gp2gp-data/Mar-2021.csv.gz\n",
      "68.1787397930002\n",
      "Now Concatenating all months of data\n",
      "4.794414346999474\n"
     ]
    }
   ],
   "source": [
    "all_messages=[generate_single_frame(file) for file in full_filenames]\n",
    "\n",
    "print('Now Concatenating all months of data')\n",
    "a=time.perf_counter()\n",
    "all_messages=pd.concat(all_messages,axis=0)\n",
    "print(time.perf_counter()-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90aad855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now constructing full interactions\n",
      "146.62699596099992\n",
      "Now Grouping by conversation\n",
      "83.8268605309986\n",
      "Now Saving Data\n"
     ]
    }
   ],
   "source": [
    "print('Now constructing full interactions')\n",
    "a=time.perf_counter()\n",
    "all_messages_listed=all_messages.merge(all_messages[['GUID','interaction_name']].rename({'interaction_name':'interaction_response'},axis=1),left_on='messageRef',right_on='GUID',how='left')\n",
    "all_messages_listed['interaction_response']=all_messages_listed['interaction_response'].fillna(\"\")\n",
    "all_messages_listed['interaction']=all_messages_listed['interaction_response']+all_messages_listed['interaction_name']\n",
    "all_messages_listed[\"messages\"] = list(zip(all_messages_listed[\"supplier_type\"], all_messages_listed[\"interaction\"], all_messages_listed[\"jdiEvent\"], all_messages_listed[\"time_of_message\"] ))\n",
    "all_messages_listed[\"messages\"] = all_messages_listed[\"messages\"].apply(list)\n",
    "all_messages_listed=all_messages_listed[[\"conversation_id\", \"messages\"]]\n",
    "all_messages_listed\n",
    "print(time.perf_counter()-a)\n",
    "\n",
    "print('Now Grouping by conversation')\n",
    "a=time.perf_counter()\n",
    "full_field_data=all_messages_listed.groupby('conversation_id')['messages'].apply(list)\n",
    "print(time.perf_counter()-a)\n",
    "\n",
    "if overwrite_files:\n",
    "    print('Now Saving Data')\n",
    "    pd.DataFrame(full_field_data).to_parquet(f's3://prm-gp2gp-data-sandbox-dev/extra-fields-data-from-splunk/Sept_20_Feb_21_conversations_extended_interaction_messages_with_time.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df293830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['requestor', 'req start', '', '0.0'],\n",
       " ['sender', 'req complete', '', '10.729000000000001'],\n",
       " ['sender', 'req start ack', '', '11.616000000000001'],\n",
       " ['requestor', 'req complete ack', '', '18637.791']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_field_data[\"000009F9-DF15-4597-9218-4024D7A79145\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54a8d0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations_extended_interaction_messages_with_time_file_name = 's3://prm-gp2gp-data-sandbox-dev/extra-fields-data-from-splunk/Sept_20_Feb_21_conversations_extended_interaction_messages_with_time.parquet'\n",
    "transfers_with_time_messages = add_messages_parquet_to_transfers(conversations_extended_interaction_messages_with_time_file_name, transfers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e724616",
   "metadata": {},
   "source": [
    "### How long does it take for a sender request complete message to arrive from request start message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f15b7c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a time if sender request completed message\n",
    "transfers_with_time_messages[\"Sender req complete times/s\"] = transfers_with_time_messages[\"messages\"].apply(lambda messages: [message[3] for message in messages if message[0:3] == ('sender', 'req complete', '')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c41474d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfers_with_time_messages[\"Min sender req complete time/s\"] = transfers_with_time_messages[\"Sender req complete times/s\"].apply(lambda times: min(times) if len(times) > 0 else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21e63120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hours\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count     443.742222\n",
       "mean        1.992413\n",
       "std        43.934522\n",
       "min         0.000338\n",
       "25%         0.002464\n",
       "50%         0.004159\n",
       "75%         0.008186\n",
       "max      4395.096212\n",
       "Name: Min sender req complete time/s, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Hours\")\n",
    "(transfers_with_time_messages[\"Min sender req complete time/s\"].dropna().astype(float).describe())/3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3d09381",
   "metadata": {},
   "outputs": [],
   "source": [
    "fourteen_days_in_seconds = 14 * 24 * 60 * 60\n",
    "percent_sender_req_complete_received_before_14_days = (transfers_with_time_messages[\"Min sender req complete time/s\"].astype(float) <= fourteen_days_in_seconds).mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d46afc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3.4% transfers where a sender req complete message is received after 14 days\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {(100 - percent_sender_req_complete_received_before_14_days).round(2)}% transfers where a sender req complete message is received after 14 days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37538829",
   "metadata": {},
   "source": [
    "### Is integration possible without sender request complete message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43b4d673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- anything that did integrate at some point\n",
    "transfers_with_time_messages[\"integrated\"] = transfers_with_time_messages[\"messages\"].apply(lambda messages: True in [True for message in messages if (message[0:3] == (\"requestor\", \"req complete ack\", '')) | (message[0:3] == (\"requestor\", \"req complete ack\", \"15\"))])\n",
    "transfers_with_time_messages[\"Contains sender req complete\"] = transfers_with_time_messages[\"messages\"].apply(lambda messages: True in [True for message in messages if message[0:3] == ('sender', 'req complete', '')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81e682a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>integrated</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Contains sender req complete</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>53490.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>64271.0</td>\n",
       "      <td>1533201.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "integrated                      False      True \n",
       "Contains sender req complete                    \n",
       "False                         53490.0        NaN\n",
       "True                          64271.0  1533201.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfers_with_time_messages.pivot_table(index=\"Contains sender req complete\", columns=\"integrated\", aggfunc=\"count\", values=\"conversation_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db88e99",
   "metadata": {},
   "source": [
    "The transfer can only integrate when there is sender req complete message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef5b3af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49c017f2",
   "metadata": {},
   "source": [
    "## Part B\n",
    "\n",
    "**We also believe that** reclassifying these transfers as Failed, will classify the transfers with fatal Sender errors as failures, and therefore make the following redundant: \n",
    "**We will know this to be true when** we can see that any transfers that would be classified as Failures because they contain these errors, do not contain the Request Completed message. \n",
    "\n",
    "### Scope\n",
    "\n",
    "1. Use 7 months of Spine Parquet files (Sept-2020 to Mar-2020 s3://prm-gp2gp-data-sandbox-dev/transfers-sample-5/)\n",
    "2. Re-label transfers which would be considered failures due to sender error as “Failed due to Sender Error”\n",
    "3. Add new status label for transfers not containing Request Completed Message\n",
    "4. Compare change in status for all “Failed due to Sender Error Messages”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764012a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfers[\"status\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbbb79d",
   "metadata": {},
   "source": [
    "## Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1e64af",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations_extended_interaction_messages_file_name = 's3://prm-gp2gp-data-sandbox-dev/extra-fields-data-from-splunk/Sept_20_Feb_21_conversations_extended_interaction_messages.parquet'\n",
    "transfers_with_message_list = add_messages_parquet_to_transfers(transfers_with_time_file_name, transfers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1674c987",
   "metadata": {},
   "outputs": [],
   "source": [
    "sender_req_completed_message = ('sender', 'req complete', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555f670e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfers_with_sender_req_completed_bool = transfers_with_message_list[\"messages\"].apply(lambda messages: sender_req_completed_message in messages)\n",
    "transfers_with_message_list_and_new_status = transfers_with_message_list.copy()\n",
    "transfers_with_message_list_and_new_status[\"New Status\"] = transfers_with_message_list_and_new_status[\"status\"]\n",
    "transfers_with_message_list_and_new_status = transfers_with_message_list_and_new_status.rename({\"status\": \"Old Status\"}, axis=1)\n",
    "transfers_with_message_list_and_new_status.loc[~transfers_with_sender_req_completed_bool, \"New Status\"] = \"Failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f96722",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfers_with_message_list_and_new_status.groupby(by=[\"Old Status\", \"New Status\"]).agg({\"conversation_id\":\"count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e918c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "old_status_sender_error_failure_bool = transfers_with_message_list_and_new_status[\"Old Status\"] == \"Failed Due To Sender Error Code\"\n",
    "new_status_sender_error_failure_bool = transfers_with_message_list_and_new_status[\"New Status\"] == \"Failed Due To Sender Error Code\"\n",
    "transfers_with_message_list_and_new_status.loc[old_status_sender_error_failure_bool & new_status_sender_error_failure_bool, \"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c76e048",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
